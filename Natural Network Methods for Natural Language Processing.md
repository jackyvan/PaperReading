## 有趣的问题：

1.如何理解distributed&distributional？

## chapter-2: learning basics and linear models

## chapter-1: introduction

11.[distributed&distributional](https://zhuanlan.zhihu.com/p/22386230)

(1) word-context matrix approach

(2) a neural language-modeling inspired word-embedding algs

10. feed-forward neural language model archs.

9.advanced topics: recursive networks for modeling trees, structured prediction models, mtl.

**8.rnn for modeling sequences and stacks.**

7. a series of works managed to obtain imporved syntactic parsing results by simply replacing the linear

model of a parser with a fully connected feed-forward network.

6.structured data of arbitrary sizes: sequences and trees. recurrent/recursive archs will capture

regularities while perserving a lot of the structural information.

5.conv for nlp

a.expect to find strong local clues regarding class membership, but these clues can appear in different

places in the input. We would like to learn that certain sequences of words are good indicators of
 
 the topic.
 
 b.learning informative ngram patterns
 
 **c.hash-kernel as alternative method**
 
 
 
4.mtl&semi-supervised learning

mtl: learning from related problems

semi-supervised: learning from external, unannotated data

3.two important archs in nlp

feed-forward networks(conv: local patterns) and recurrent/recursive networks.

2.view of dl.

a. mathematical view

b.brain-inspired view

1.why nlp is hard?

a. ill-defined and unspecified set of rules

for example, we can't find the function as f(red)=pink, but it's easy to image and others.

b.discrete, compositional(char, word, sentence, paragraph, doc and so on), and sparse

## abstract

1.words representation: vector-based&symbolic representations
