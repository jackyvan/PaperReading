### NLG

1.《Unifying Human and Statistical Evaluation for Natural Language Generation》NAACL2019

motivation:

(1)quality: 人类评估可以评估质量，但是无法评估diversity；bleu&rouge在评估quality方面做的比ppl好，但是还是比human弱，同时在评估diversity方面也是很弱。

(2)diversity： ppl可以评估，但是无法评估quality；

(3)一个unified framework：（最优错误率）预测一个sent来自human还是machine

问题：如何证明huse（human unified with statistical evaluation）是work的？

结论：huse=quality+diversity

2.[Are generative models good enough?](https://blog.singularitynet.io/are-generative-models-good-enough-a-case-study-on-class-modeling-9a57c91ddcae)

用**一个简单的例子**讲了几个generative模型的问题，包括生成模型和判别模型的区别和联系；vae/gan等；

### Transformer

1.《Training Tips for The Transformer Model》基于英语-捷克语的语料，用transformer做翻译模型。使用tensor2tensor框架，对任务中的各个参数进行了实验探索。

总结如下：

#### （1）checkpoint averaging：几乎总是有稳定的提升。

#### （2）resumed training：

使用场景：

（1）由于硬件故障，得重新训练；

（2）有更好的超参搜索想法

（3）domain adaption

缺点：

（1）并不记录训练数据中的位置。也可以利用这个特性做checkpoint averaging，因为训练数据不同，理论上会影响到最后的训练结果；

（2）tensorboard中的relative time(wall-clock time)不对齐。表现为新的损失曲线和原来的曲线不对齐；

#### (3)learning rate and warmup step

（1）大batch学习是一个开放问题，主要解决的是当batch变大后，学习率应该怎样变化？当使用warmup时，也需要考虑该问题。

（2）当由单gpu切换到多gpu时，学习率保持单gpu上的就可以；同时warmup并不能显著改善最后的度量，但是有助于收敛和发散过程的控制。

（3）缓解训练过程中的divergence：更小的学习率；gradient clipping；more warmup steps。

#### （4）number of gpus

多卡能够带来更快收敛的同时，提升度量指标。

#### （5）batch size：越大越好

#### （6）max\_length

#### （7）model size

(1)prefer the big over the base model

(2)use transformer\_tiny to fast debugging

#### (8) training data size

smaller and cleaner v.s. bigger and noisier

#### (9)training data preprocessing

（1）在t2t-datagen之前，过滤过长的句子，这样在tfrecord阶段可以节约很多时间。

（2）数据处理规范：train/dev/test；过滤掉数字和特殊符号；过滤掉过长的句子；将整个流程分批次进行；

### Graph Convolutional Network

5.一个[tutorial](https://mp.weixin.qq.com/s?__biz=MzUzODcyNjY5Ng==&mid=2247484063&idx=1&sn=edfc6c55240340d9c1468202d40074b3&chksm=fad21ad8cda593ce216120048fcd3a38006060e8ba4eacc22b1fee4e35f9e1930c71cdb5df75&mpshare=1&scene=23&srcid=%23rd)的阅读笔记

[相关文章](https://mp.weixin.qq.com/s?__biz=MzU0ODk1MTcwNA==&mid=2247483678&idx=1&sn=46a8bdd31fe4db8f64a4bf81b4656bdc&chksm=fbb602e9ccc18bffe659e695f274cf0292a949ae9226ff5c19ff4ba6047fc62fb194a8129daa&scene=21#wechat_redirect)

图研究的内容：

（1）经典图算法

（2）概率图模型

（3）图神经网络

最近的nlp相关的工作：

（1）gcn用于文本分类，aaai2019

（2）用于srl，emnlp2017

作者的一个观点：

![imgx](http://wx3.sinaimg.cn/mw690/aba7d18bly1g2ipz8gdctj21b60ac7gu.jpg)



4.《Graph Attention Networks》，ICLR2018

在nlp的问题中，看到了rnn，cnn的效果，同时更看到了attention的效果。在这篇文章前，很早就有人做graph的问题，比如基于kernel的方法，基于conv的方法(gcn)等，那么一个很直觉的想法是将attention引入到graph的问题中去。

主要贡献：

使用self-attention（multi-head）做基于graph的transductive和inductive两类问题。给定node的feature后，可以计算与neighbors的attention得分，然后attend到node feature中去。

在保证效果不错的同时，具备attention的优点。不过，结合graph问题本身，之前的一类基于spectral的方法注重eigen value/vector的计算，使用attention不会有前者的巨大的计算开销。不过，和gcn对比时，仍旧可以讨论在nlp中的问题，**conv和attention选哪个？**

后续工作：

（1）受限于目前库的实现，导致现有方案的batch能力较弱。可以从两个方面来做：第一:完善现有库，支持更高阶的稀疏矩阵运算；第二：对算法本身的修改

（2）通过skip connection加深网络层，因为文章中的实验仍旧是在两层结构上做的。不深的网络结构，意味着无法充分发挥gpu的效能。在gcn的那篇工作中，作者是加了residual的，确实证明能够进一步提升效果，不过太浅的话，很容易会遇到degration的现象。

（3）减少大规模图中，分布式场景下的冗余计算。

想法：在graph的问题中，cnn，rnn，attention等都已经出现了相关的工作，不过显然cnn和attention的相关工作大概两年前刚刚开始。起码在深度上还是相当的浅，虽然有一些对深度的探讨工作。可能graph的场景比起nlp其他的任务场景来说，较少，导致相关数据集也不是很多，更不用提一些比较令人惊讶的在某些经典任务上的模型了。kg的相关积累正在进行中，可能一些工作也正在尝试把gcn或者gan的东西用进去吧，期待有新的工作进展。不过，graph早期的基于spectral的方法在理论上是非常漂亮的，包括一些基于kernel的方法。




3.《Large-Scale Learnable Graph Convolutional Networks》,KDD2018

整体上看，要把conv用到graph结构上，一方面要考虑**是否conv在cv和nlp中的常见相关应用能否确实直接迁移到graph结构上？**另一方面要考虑**如何结合graph结构做针对性的设计？**

主要工作：

（1）**为了可以构建deeper的网络结构**，提出learnable graph convolutional layer(LGCL)。主要思想是将相邻节点feature聚合的粒度变小，基于每个node的每个feature dim考察。当切分粒度有node整体变为node的每个feature dim时就更容易解决对齐的问题，同时不损失提取的feature质量。

（2）**为了实现在大规模场景下提高训练的高效性**，提出sub-graph selection process。大规模场景下，要尽可能地减少数据量，因此可以通过采样的方式来进行。从一个大的graph中采样一部分edeg和node形成一个小的graph。具体思路是任意给定初始节点，然后用bfs去寻找一阶近邻节点，之后采样。之后从这些采样后的node中继续前述过程，直到进行给定迭代数。类似于剥洋葱，一层层局部的去采样一些node，这样整体性不会被打破。

后续工作：

（1）graph上的pooling操作。

（2）将该工作用于文本数据（文中的实验都是在引用关系网络上做的）。

总结：整体上看，实验结果相比于前人也不是特别的显著。但是方法简单，同时能够通过实验证明确实是有效的。文章写的很好。

2.《semi-supervised classification with graph convolutional network》,iclr2017

应该是最早提出gcn的文章，文章给出了tf的代码，后来作者又给出了pt，keras等各个版本下的代码。rgb以写faster-rcnn来打发无聊的时候，不知道kipf是不是靠写gcn来排遣空闲。这里的应用场景是半监督分类。典型的一个例子是论文分类+论文引用信息。论文引用构成了一个graph，graph包含的丰富的信息。比如，相关类别的论文有更大的可能应用同类别的论文。因此如何将graph的信息融合进分类任务从而帮助分类效果的提升就是一个关键了。因为需要处理的数据是graph形式的，因此就多了一些特色。类似的处理场景在nlp下有kg知识的融合，之前做的一个事情是简单的将kg中的三元组concat后embedding到网络中，从而确实提升了对话效果，能否将gcn用于kg是一个值得思考的问题。由于是最早的工作，作者做了两层的gcn，从结构上的设计来看，更像是一个前馈网络的形式。这篇文章提供了应用gcn的原始建模思路，值得借鉴。这里给出[pygcn的代码地址](https://github.com/tkipf/pygcn)，代码基于一个论文引用网络做文本分类，结构非常清晰易懂。

1.《Multi-Label Image Recognition with Graph Convolutional Networks》

0.《Graph Convolutional Networks for Text Classification
》

想法：0和1是两篇关于gcn的文章。印象中18年年底和19年年初，gcn的综述文章出了至少有三篇。0的思路是这样的，将文本分类转化为一个graph上的node分类问题。很显然，这里的node要满足\#node>=c，其中c是类别总数。既然是graph，不仅需要node，还需要edge。在文章中，node有两类组成，分别是document和word，那么edge的信息就是document和word的关联关系的表示了。比较容易想到的有经典的tf-idf，co-currence等，在论文中词与词之间的关联关系表示用了pmi，也是一个统计量。这样的话，图就构造结束了。对文档分类，损失函数只考虑node类型为document的就可以了。

1是cvpr2019的文章。multi-label问题的关键之一是要考虑：**标签依赖**。那么用graph来建模这种依赖关系好了，这样还是node分类问题，edge表示的关联信息来自对数据集中标签的统计量，典型的co-currence。不过仅仅通过co-currence得到的edge关系在数值上是不完美的，文章对edge关系进行了基于weight的修正。文章堆叠了gcn，最后输出的矩阵维度为\#node\*#d。对图片的特征提取就是一个普通的cnn网络，最后输出的矩阵维度为\#d，两类输出做点积就是类别预测向量了(
\#node=\#c)。

扩展：针对细粒度情感分类问题（如ai challenger2018赛道），或许也可以用gcn来做。对评论文本的特征提取用任意一个好的结构来做。将标签展开成20个label，显然label之间是有着依赖关系的，比如性价比和折扣力度是正相关的。对label本身embedding做为node feature，edge信息如何算？一种思路是计算情感值的差，这样的话，edge的信息就是一个多维向量了。剩下的事情和1差不多。不过，由于标签存在层次结构，因此或许可以利用一下。此外，attention能否在这种场景下得到应用也值得思考。


