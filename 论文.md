### 个人觉得值得精读/反复读的一些文章

1.《BERT_Pre-training of Deep Bidirectional Transformers for Language Understanding》

2.《Attention Is All You Need》

3.《Summarization with Pointer-Generator Networks》

4.《Universal Language Model Fine-tuning for Text Classiﬁcation》

### EMNLP2019论文选读（浏览了一些自己感兴趣方向的文章）

总结：EMNLP的文章读起来八股气息要弱一些，文章类型更加丰富，个人觉得是好现象。不过收一些个人感觉明显质量有问题的工作也是有点奇怪。下述文章只是看题目觉得可看，就大致浏览了一下。

1.《Text Summarization with Pretrained Encoders》

思路：用bert去fine-tune句子级的embedding，然后对sentence做binary classification(要不要？)。实际上，直接对token做binary classification未尝不可，不过相比前者，semantic的粒度确实小了很多。一如很多工作，这次bert后不加bilstm，不加crf，加self-attention层了。额，可以一直加，但是不要搞得像CV的一些工作就行。比较有启发的是，如何构造输入进行sent embedding的学习。bert不是只可以一个[CLS]和两个[SEP]吧。

2.《Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks》

思路：当输入两个句子直接进bert得到sent embedding的问题在于，当句子比较长的时候就有点尴尬了。一个比较直接的思路是，每个bert喂一个sent，用两个bert做embedding，然后加融合层，比如cosine之类的操作等，整体上是siamese的结构。除此之外，文章也提出了triplet loss的应用。嗯，整体上的工作和人脸领域的一类工作很类似了。

3.《Neural Text Summarization: A Critical Evaluation》

个人较喜欢的一个工作，相对务虚，不过指出的问题需要思考。文章认为：**整体上神经关系抽取近期的工作基本处于停滞状态。**比如，有些数据集上，SOTA只比直接从文章中抽取前三句话好一点点。原因有三个方面：（1）数据集的问题。某些数据集只给定一篇文章+一个参考摘要，除此之外，没啥额外的信息了。真实场景下可不是这样的哦。也就是说，现在的数据集构造还是离真实场景有点远。（2）评估指标的问题。已经是生成式模型的老问题了，除了大家知道的一些弊端，还有对**事实性错误的评估**。（3）模型到底学到的是个啥？

4.《Attending to Future Tokens for Bidirectional Sequence Generation》

思路：用bert做序列生成。输入源句子+目标句子用于训练，其中目标句子随机选一些token用placeholder替换。扩展一下，关于bert用于生成，最近印象中有一些工作。第一：类似于本文的思路。第二：seq2seq中将bert作为encoder，decoder单独训练(模型可以灵活选择)。第三：decoder端用bert。第四：更加广义地应用，比如做extractive摘要的工作。

5.《Dynamic Past and Future for Neural Machine Translation》

思路：用于Capsule Network做机器翻译。

6.《Learning to Recognize Discontiguous Entities》

解决的问题：命名实体识别中，识别不连续的实体。

7.《ner and pos when nothing is capitalized》

解决的问题：在NER和POS任务中，**大小写很重要。**

8.《On NMT Search Errors and Model Errors: Cat Got Your Tongue?》

解决的问题：组合beam search和depth-first search做精确推断用于NMT任务。作者在文末明确指出：该方法不实用，但是可以帮助理解现有一些NMT的Trick可能并不能完全解决一些特定的问题。

### BERT

3.《Revealing the Dark Secrets of BERT》

model pruning and finding an optimal sub-architecture reducing data repetition;

主要方法：对attention weight 进行可视化

主要的想法：disabling self-attention heads（应该是一个比较普遍的结论）

2.《Universal Language Model Fine-tuning for Text Classification》

**非常好的工作，相信可以对更好地利用bert做fine-tuning有很好的启发。**

主要思路(对ULMFiT的使用包含三个阶段， 基于3层lstm)：

(1) LM pre-training

(2) LM fine-tuning（不同层用不同的学习率，2.6倍差异）

(3) Classifier fine-tuning（gradual unfreezing，每个epoch都只unfreeze掉一部分layer）

现在多数情况下，大家对于bert用于fine-tune的过程通常是没有第二个阶段的。


1.《Visualizing and Understanding the Effectiveness of BERT》，**2019.08.15**

主要思路：可视化loss landscape和optimization trajectories

主要结论：

（1）pre-training可以到达一个好的优化初始化点，使得下游任务的训练到达wider optima和easier optimization；另一方面虽然bert是over-parameterized的，但是robust to overfitting。

（2）fine-tuning可以获得更好的泛化性能（flat and wide optima），同时可以观察到traning loss surface和generalization errro的一致性；

（3）bert的低层在fine-tuning的过程中more invariant，这意味着学习到了more transferable representation of language。

关于第三点的具体展开：《BERT Rediscovers the classical NLP pipeline》

lower layers: most local syntactic phenomena

higher layers: more complex semantics


主要建议：

**开发一些好的fine-tuning的算法，使得能够使下游任务的最优点收敛到一个wider and more flat optima。**

### Classification

3.《Learning Named Entity Tagger using Domain-Specific Dictionary》

用于ner任务，两个方法。这篇文章和**AutoPhrase**（**语料库+wikipedia+pos，整体上偏规则方法**）的作者基本相同。

第一：fuzzy crf。利用crf挖掘multi-label的信息；

第二：AutoNer。新的标注范式，用两个token是否连接来标注。大概思路如下：

输入一句话：艾耕科技由高榕投资。

构造数据如下：

[S] 艾 1
 
 艾 耕 1
 
 耕 科 1
 
 科 技 1
 
 技 由 0
 
 由 高 0
 
 高 榕 1
 
 榕 投 0
 
 投 资 0
 
 资 [E] 0
 
 [E] 。 0
 
 剩下的事情就是labeling了。



2.《BERT for Joint Intent Classification and Slot Filling》

个人非常喜欢的思路。

intent classification：给一句话打标签，多分类，必要时multi-label；比如一句话“艾耕科技由高榕投资。”，类别标签是：**是否是融资新闻？**

slot filling：slot对应字段，比如**融资机构，投资机构**等，filling是找到对应字段类型的值，比如**艾耕科技，高榕**等。

思路：[CLS]做intent classification；slot filling用sequence labeling做；

思路扩展：

（1）两个模型。一个做分类，一个做labeling；

（2）一个模型做，但是两个任务是异步的；

（3）multi-task

（4）就是上面的思路了，同步做；

目标函数是最大化条件概率：p(y(i), y(s)|x) ，其中y(i)是intent的label的softmax分布，y(s)是slot的label的softmax分布（有多个slot）。本质上等价于cross\_entropy的最小化，同为分类问题。


1.《A Simple and Effective Approach for Fine-Tuning Pre-trained Word Embeddings for Imporved Text Classification》

一个分类的Trick：在对word embedding进行fine-tuning的时候，每个example添加类别的信息。fine-tuning的model是doc2vec。

0.《Bag of Tricks for Efﬁcient Text Classiﬁcation》

fastText的工作。

（a）

Q:linear classiﬁers do not share parameters among features and classes. This possibly limits their generalization in the con- text of large output space where some classes have very few examples.

A:low rank matrices, multi layer neural networks

（b）**bow对word order不敏感，需要显式的考虑order，因此有了bag of n-grams，不过仍旧是只考虑了local order。**

（c）两个实现上的技巧：hierarchical softmax， n-grams的hash trick

### Learning to Rank

#### 1.RankNet，《Learning to rank using gradient descent》,ICML2005

主要目的：学习一个Ranking Function

训练：给定一个doc list，构建example = (doc1，doc2)。如果doc1和query更相关，则label=1；否则label = 0。

测试：给定一个doc list，通过构建pair list，得到label list，最后可以得到一个ranking结果。

#### 2.LambdaRank, 《Learning to Rank with Nonsmooth Cost Functions》,NIPS2006

主要目的：损失函数中添加对排序指标的优化项。

#### 3.LambdaMART

Lambda+MART(GBDT,梯度提升树)

#### 4.ListNet,《Learning to Rank: From Pairwise Approach to Listwise Approach》

上述三篇都是input为pair的方法，这篇工作是input为list的方法。一般认为效果上，后者要优于前者。

相关资源：

1.[PTL2R](https://github.com/ptl2r/ptl2r.github.io)

基于PyTorch，实现了多种排序学习的算法。

2.[RankNet的实现](https://github.com/ShaoQiBNU/RankNet)

### Knowledge Graph

3.ACL2018,《Improving Entity Linking by Modeling Latent Relations between Mentions》

**goal:** link each mention to an entity in a KB.

**candidate selection:** local and global modeing


2.《OAG: Toward Linking Large-scale Heterogeneous Entity Graphs》,KDD2019


1.《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》,AAAI2019


### others

6.《data decisions and theoretical implications when adversarially learning fair representations》

主要贡献: we use an adversarial training procedure to remove information about the sensitive attribute from the latent representation learned by a neural network.

![img](http://wx1.sinaimg.cn/mw690/aba7d18bgy1g2o8funikij20j40dwgmo.jpg)

5.《the sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network》,1999年trans on information theory的一篇文章

**Results in this paper show that if a large neural network is used for a pattern classiﬁcation problem and the learning algorithm ﬁnds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights.**

相关论文，《Real numbers, data science and chaos: How to ﬁt any dataset with a single parameter》


4.《reducing multiclass to binary：a unifying approach for margin classifiers》

**传统方法：**

（1）each class is compared against all others

（2）all pairs of classes are compared to each other

(3) output codes with error-correcting properties are used, [文献](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/lecture_notes/ecoc/ecoc.pdf)，周志华老师的书上也有介绍，不过更常见的多是前两种。

3.《classifying relations by ranking with convolutional neural networks》

**主要贡献：**

（1）提出了一个新的pairwise rank loss用于减少人工类的影响。（我们之前也有一个工作将改进的pairwise rank loss用于图片质量评价）

（2）提供了一个证据：cnn+rank loss > cnn+softmax

（3）在关系分类任务中，如果仅仅考虑两个nominal之间的text，使用word embedding就可以sota

周博文挂名的文章看的几篇都是思路比较清晰的，方法上虽然看起来不太复杂，但是work。同时，有必要关注一下，logistic loss， rank loss， margin loss等在nlp领域中的应用，不一定logistic loss去dominate所有的情况，印象中cv的某些任务上，margin就表现的比较好。rank loss印象中在ir上应用较广，但是loss之间并非绝对的独立，对他们之间关系的思考也是一个有趣的方向，估计已经有一些工作了。


2.《dropout training as adaptive regularization》,percy liang等人的文章

主要贡献：对于glm，dropout可以认为是一种自适应正则化的技术，同时建立了与adagrad的联系。在dropout，regularization和adagrad的联系基础上，提出了一个semi-supervised算法，该算法使用没有标签的数据可以建立一个更好的自适应的regularizer。

we show that the dropout regularizer is first-order equivalent so an L2 regularizer applied after scaling

the features by an estimate of the inverse diagonal Fisher information matrix.

1.《Using Pre-Training Can Improve Model Robustness and Uncertainty》

Through extensive experiments on **label corruption, class imbalance, adversarial examples, out-of-distribution detection, and conﬁdence calibration**, we demonstrate large gains from pre-training and complementary effects with task-speciﬁc methods.

### Language Model

3.《The Curious Case of Neural Text Degeneration》

motivation：机器生成的语言的prob分布（锯齿状且更加平稳）和人生成的语言的prob分布是不一致的；

solution：一种decoder端的策略。

2.《Using the Output Embedding to Improve Language Models》

weight typing技术。lm的input和output embedding共享。

优点：减少模型容量到一半；正则化；

印象中多个lm的实现都支持相同做法。

 1.《think again network, the delta loss, and an application in language modeling》
 
 lm在penn treebank上的新的sota，通过在rnn的hidden输出上再添加一层recurrent机制，使得性能提升；同时提出了一个新的损失函数，但是实验上并没有提升；虽然实现了新的sota，但是个人认为模型会相对较大，需要保留较多的历史信息。


### NLG

5.《Comparison of Diverse Decoding Methods from Conditional Language Models》

梳理了围绕beam search改进的各种方法，可以在不损失quality的同时，增加diversity。

4.《A Simple Theoretical Model of Importance for Summarization》

几个用于评估摘要的指标：**Redundancy**, **Relevance**, and **Informativeness**.

3.《Towards Knowledge-Based Personalized Product Description Generation in E-commerce》，kdd2019

主要贡献：基于seq2seq（transformer），融合用户属性，商品属性和知识图谱，生成个性化的商品描述。

2.《challenging common assumptions in the unsupervised learning of disentangled representation》

主要内容：如题

主要特色：用大量的实验来challenge一个common assumption，主要讨论的是vae相关变种模型。

想法：对vae和gan以及正则化流相关的工作没有做过深入思考，因此这篇文章自认为并不是读的特别明白，后续有相关工作了需要进一步思考。

1.《Unifying Human and Statistical Evaluation for Natural Language Generation》NAACL2019

motivation:

(1)quality: 人类评估可以评估质量，但是无法评估diversity；bleu&rouge在评估quality方面做的比ppl好，但是还是比human弱，同时在评估diversity方面也是很弱。

(2)diversity： ppl可以评估，但是无法评估quality；

(3)一个unified framework：（最优错误率）预测一个sent来自human还是machine

问题：如何证明huse（human unified with statistical evaluation）是work的？

结论：huse=quality+diversity

2.[Are generative models good enough?](https://blog.singularitynet.io/are-generative-models-good-enough-a-case-study-on-class-modeling-9a57c91ddcae)

用**一个简单的例子**讲了几个generative模型的问题，包括生成模型和判别模型的区别和联系；vae/gan等；

### Transformer

3.《Attention is all you need》,nips2017

从整体上看，transformer的encoder端就是一堆linear层的堆叠，添加**norm layer和dropout**，在最后的pooling层添加了tanh的activation函数；

思想：完全依赖attention机制获取input和output端的全局依赖。

end-to-end memory networks are based on a **recurrent attention mechanism** instead of **sequence-aligned recurrence**. Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.

Encoder: LayerNorm（x+SubLayer(x)）；

Decoder: 是三个子层，self-attention需要添加mask；

问题：

（1）transformer中，scaled dot-production attention可以用additive attention代替吗？能和不能的原因是什么？

（2）attention(q,k,v)=softmax(qk^t/sqrt(d_k))v中，为啥需要这个**scaling factor**？

（3）为什么self-attention是可以work的？或者说self-attention的优点是什么？

a.computational complexity per layer

b.amount of computation that can be parallelized(measured by the minimum number of sequential operations required)

(个人认为可以并行优化的地方：第一：分别计算q，k，v的时候；第二，multi-head scaled dot-product attention)

c.the path length for long-range dependencies(from one position to another position)

d.interpretable(syntactic and semantic)

（4）transformer中使用的regularization技术有哪些？

a. apply dropout to the output of each sub-layer

b. apply dropout to the sums of the embeddings and positional encodings in both the encoder and decoder stacks

c. label smooothing: this hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.



2.《SpanBERT: Improving Pre-training by Representing and Predicting Spans》

contribution：

（1）masking contiguous random spans, rather than random tokens

（2）training the span boundary representations, encourage the model to store this span-level information at the boundary token.

一张图解释：

![img](http://wx3.sinaimg.cn/mw690/aba7d18bgy1g5d25mjbdqj214y0gwacy.jpg)

个人想法：类似工作很多了，印象中今年MSRA在wmt2019取的好多第一的工作《MASS: Masked Sequence to Sequence Pre-training for Language Generation》ICML2019;

百度的[ERNIE](https://github.com/PaddlePaddle/ERNIE)的工作；

我的博客[神经关系抽取](https://zhpmatrix.github.io/2019/06/30/neural-relation-extraction/)中的一些工作；

我们组在生成方面的一些尝试等。

总之，个人对类似工作已经提不起兴趣了。

1.《Training Tips for The Transformer Model》基于英语-捷克语的语料，用transformer做翻译模型。使用tensor2tensor框架，对任务中的各个参数进行了实验探索。

总结如下：

#### （1）checkpoint averaging：几乎总是有稳定的提升。

#### （2）resumed training：

