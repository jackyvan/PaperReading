### Classification

3.《Learning Named Entity Tagger using Domain-Specific Dictionary》

用于ner任务，两个方法。这篇文章和**AutoPhrase**（**语料库+wikipedia+pos，整体上偏规则方法**）的作者基本相同。

第一：fuzzy crf。利用crf挖掘multi-label的信息；

第二：AutoNer。新的标注范式，用两个token是否连接来标注。大概思路如下：

输入一句话：艾耕科技由高榕投资。

构造数据如下：

[S] 艾 1
 
 艾 耕 1
 
 耕 科 1
 
 科 技 1
 
 技 由 0
 
 由 高 0
 
 高 榕 1
 
 榕 投 0
 
 投 资 0
 
 资 [E] 0
 
 [E] 。 0
 
 剩下的事情就是labeling了。



2.《BERT for Joint Intent Classification and Slot Filling》

个人非常喜欢的思路。

intent classification：给一句话打标签，多分类，必要时multi-label；比如一句话“艾耕科技由高榕投资。”，类别标签是：**是否是融资新闻？**

slot filling：slot对应字段，比如**融资机构，投资机构**等，filling是找到对应字段类型的值，比如**艾耕科技，高榕**等。

思路：[CLS]做intent classification；slot filling用sequence labeling做；

思路扩展：

（1）两个模型。一个做分类，一个做labeling；

（2）一个模型做，但是两个任务是异步的；

（3）multi-task

（4）就是上面的思路了，同步做；

1.《A Simple and Effective Approach for Fine-Tuning Pre-trained Word Embeddings for Imporved Text Classification》

一个分类的Trick：在对word embedding进行fine-tuning的时候，每个example添加类别的信息。fine-tuning的model是doc2vec。

0.《Bag of Tricks for Efﬁcient Text Classiﬁcation》

fastText的工作。

（a）

Q:linear classiﬁers do not share parameters among features and classes. This possibly limits their generalization in the con- text of large output space where some classes have very few examples.

A:low rank matrices, multi layer neural networks

（b）**bow对word order不敏感，需要显式的考虑order，因此有了bag of n-grams，不过仍旧是只考虑了local order。**

（c）两个实现上的技巧：hierarchical softmax， n-grams的hash trick

### Learning to Rank

#### 1.RankNet，《Learning to rank using gradient descent》,ICML2005

主要目的：学习一个Ranking Function

训练：给定一个doc list，构建example = (doc1，doc2)。如果doc1和query更相关，则label=1；否则label = 0。

测试：给定一个doc list，通过构建pair list，得到label list，最后可以得到一个ranking结果。

#### 2.LambdaRank, 《Learning to Rank with Nonsmooth Cost Functions》,NIPS2006

主要目的：损失函数中添加对排序指标的优化项。

#### 3.LambdaMART

Lambda+MART(GBDT,梯度提升树)

#### 4.ListNet,《Learning to Rank: From Pairwise Approach to Listwise Approach》

上述三篇都是input为pair的方法，这篇工作是input为list的方法。一般认为效果上，后者要优于前者。

相关资源：

1.[PTL2R](https://github.com/ptl2r/ptl2r.github.io)

基于PyTorch，实现了多种排序学习的算法。

2.[RankNet的实现](https://github.com/ShaoQiBNU/RankNet)

### Knowledge Graph

3.ACL2018,《Improving Entity Linking by Modeling Latent Relations between Mentions》

**goal:** link each mention to an entity in a KB.

**candidate selection:** local and global modeing


2.《OAG: Toward Linking Large-scale Heterogeneous Entity Graphs》,KDD2019


1.《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》,AAAI2019


### others

6.《data decisions and theoretical implications when adversarially learning fair representations》

主要贡献: we use an adversarial training procedure to remove information about the sensitive attribute from the latent representation learned by a neural network.

![img](http://wx1.sinaimg.cn/mw690/aba7d18bgy1g2o8funikij20j40dwgmo.jpg)

5.《the sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network》,1999年trans on information theory的一篇文章

**Results in this paper show that if a large neural network is used for a pattern classiﬁcation problem and the learning algorithm ﬁnds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights.**

相关论文，《Real numbers, data science and chaos: How to ﬁt any dataset with a single parameter》


4.《reducing multiclass to binary：a unifying approach for margin classifiers》

**传统方法：**

（1）each class is compared against all others

（2）all pairs of classes are compared to each other

(3) output codes with error-correcting properties are used, [文献](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/lecture_notes/ecoc/ecoc.pdf)，周志华老师的书上也有介绍，不过更常见的多是前两种。

3.《classifying relations by ranking with convolutional neural networks》

**主要贡献：**

（1）提出了一个新的pairwise rank loss用于减少人工类的影响。（我们之前也有一个工作将改进的pairwise rank loss用于图片质量评价）

（2）提供了一个证据：cnn+rank loss > cnn+softmax

（3）在关系分类任务中，如果仅仅考虑两个nominal之间的text，使用word embedding就可以sota

周博文挂名的文章看的几篇都是思路比较清晰的，方法上虽然看起来不太复杂，但是work。同时，有必要关注一下，logistic loss， rank loss， margin loss等在nlp领域中的应用，不一定logistic loss去dominate所有的情况，印象中cv的某些任务上，margin就表现的比较好。rank loss印象中在ir上应用较广，但是loss之间并非绝对的独立，对他们之间关系的思考也是一个有趣的方向，估计已经有一些工作了。


2.《dropout training as adaptive regularization》,percy liang等人的文章

主要贡献：对于glm，dropout可以认为是一种自适应正则化的技术，同时建立了与adagrad的联系。在dropout，regularization和adagrad的联系基础上，提出了一个semi-supervised算法，该算法使用没有标签的数据可以建立一个更好的自适应的regularizer。

we show that the dropout regularizer is first-order equivalent so an L2 regularizer applied after scaling

the features by an estimate of the inverse diagonal Fisher information matrix.

1.《Using Pre-Training Can Improve Model Robustness and Uncertainty》

Through extensive experiments on **label corruption, class imbalance, adversarial examples, out-of-distribution detection, and conﬁdence calibration**, we demonstrate large gains from pre-training and complementary effects with task-speciﬁc methods.

### Language Model

2.《Using the Output Embedding to Improve Language Models》

weight typing技术。lm的input和output embedding共享。

优点：减少模型容量到一半；正则化；

印象中多个lm的实现都支持相同做法。

 1.《think again network, the delta loss, and an application in language modeling》
 
 lm在penn treebank上的新的sota，通过在rnn的hidden输出上再添加一层recurrent机制，使得性能提升；同时提出了一个新的损失函数，但是实验上并没有提升；虽然实现了新的sota，但是个人认为模型会相对较大，需要保留较多的历史信息。


### NLG

5.《Comparison of Diverse Decoding Methods from Conditional Language Models》

梳理了围绕beam search改进的各种方法，可以在不损失quality的同时，增加diversity。

4.《A Simple Theoretical Model of Importance for Summarization》

几个用于评估摘要的指标：**Redundancy**, **Relevance**, and **Informativeness**.

3.《Towards Knowledge-Based Personalized Product Description Generation in E-commerce》，kdd2019

主要贡献：基于seq2seq（transformer），融合用户属性，商品属性和知识图谱，生成个性化的商品描述。

2.《challenging common assumptions in the unsupervised learning of disentangled representation》

主要内容：如题

主要特色：用大量的实验来challenge一个common assumption，主要讨论的是vae相关变种模型。

想法：对vae和gan以及正则化流相关的工作没有做过深入思考，因此这篇文章自认为并不是读的特别明白，后续有相关工作了需要进一步思考。

1.《Unifying Human and Statistical Evaluation for Natural Language Generation》NAACL2019

motivation:

(1)quality: 人类评估可以评估质量，但是无法评估diversity；bleu&rouge在评估quality方面做的比ppl好，但是还是比human弱，同时在评估diversity方面也是很弱。

(2)diversity： ppl可以评估，但是无法评估quality；

(3)一个unified framework：（最优错误率）预测一个sent来自human还是machine

问题：如何证明huse（human unified with statistical evaluation）是work的？

结论：huse=quality+diversity

2.[Are generative models good enough?](https://blog.singularitynet.io/are-generative-models-good-enough-a-case-study-on-class-modeling-9a57c91ddcae)

用**一个简单的例子**讲了几个generative模型的问题，包括生成模型和判别模型的区别和联系；vae/gan等；

### Transformer

2.《SpanBERT: Improving Pre-training by Representing and Predicting Spans》

contribution：

（1）masking contiguous random spans, rather than random tokens

（2）training the span boundary representations, encourage the model to store this span-level information at the boundary token.

一张图解释：

![img](http://wx3.sinaimg.cn/mw690/aba7d18bgy1g5d25mjbdqj214y0gwacy.jpg)

个人想法：类似工作很多了，印象中今年MSRA在wmt2019取的好多第一的工作《MASS: Masked Sequence to Sequence Pre-training for Language Generation》ICML2019;

百度的[ERNIE](https://github.com/PaddlePaddle/ERNIE)的工作；

我的博客[神经关系抽取](https://zhpmatrix.github.io/2019/06/30/neural-relation-extraction/)中的一些工作；

我们组在生成方面的一些尝试等。

总之，个人对类似工作已经提不起兴趣了。

1.《Training Tips for The Transformer Model》基于英语-捷克语的语料，用transformer做翻译模型。使用tensor2tensor框架，对任务中的各个参数进行了实验探索。

总结如下：

#### （1）checkpoint averaging：几乎总是有稳定的提升。

#### （2）resumed training：

